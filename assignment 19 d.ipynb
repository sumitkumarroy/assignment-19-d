{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c09d101-3584-4044-af0f-17eedf401a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmin-max technique is a normalizing technique where all tha data points are normalized to fit between a specific range [0,1] or [-1,1]\\n\\neg: Determine X min\\u200b  and X max :\\nLet's scale the feature values [10, 20, 30, 40, 50] to the range [0, 1].\\nX min\\u200b =10\\nX max\\u200b =50\\nApply the Min-Max scaling formula:\\n\\nX ′ = X−10/50−10\\n\\u200b\\n \\nFor \\nX=10: X ′ = 10−10/50−10 =0\\nFor X=20:X ′ = 20−10/50−10\\u200b =0.25\\nSimilarly, the scaled values are [0, 0.25, 0.5, 0.75, 1].\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q1\n",
    "'''\n",
    "min-max technique is a normalizing technique where all tha data points are normalized to fit between a specific range [0,1] or [-1,1]\n",
    "\n",
    "eg: Determine X min​  and X max :\n",
    "Let's scale the feature values [10, 20, 30, 40, 50] to the range [0, 1].\n",
    "X min​ =10\n",
    "X max​ =50\n",
    "Apply the Min-Max scaling formula:\n",
    "\n",
    "X ′ = X−10/50−10\n",
    "​\n",
    " \n",
    "For \n",
    "X=10: X ′ = 10−10/50−10 =0\n",
    "For X=20:X ′ = 20−10/50−10​ =0.25\n",
    "Similarly, the scaled values are [0, 0.25, 0.5, 0.75, 1].\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93238b8c-a638-4a17-b532-1fcd99354585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nUnit Vector Technique (also known as vector normalization) scales the feature vector to have a unit norm (i.e., a length of 1). \\nDifference from Min-Max Scaling:\\n\\nMin-Max Scaling transforms the data to a specified range.\\nUnit Vector Technique scales the data such that the vector length is 1.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q@\n",
    "'''\n",
    "Unit Vector Technique (also known as vector normalization) scales the feature vector to have a unit norm (i.e., a length of 1). \n",
    "Difference from Min-Max Scaling:\n",
    "\n",
    "Min-Max Scaling transforms the data to a specified range.\n",
    "Unit Vector Technique scales the data such that the vector length is 1.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8be40180-e869-4e49-aac4-56565d93c913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPCA is a technique used for dimensionality reduction while preserving as much variance as possible. PCA transforms the data into a new coordinate \\nsystem where the greatest variances are aligned with the new axes (principal components).\\n\\nExample:\\n\\nSuppose we have a dataset with two features: height and weight. We want to reduce it to one dimension using PCA.\\n\\nStandardize the data.\\nCompute the covariance matrix.\\nCompute the eigenvalues and eigenvectors of the covariance matrix.\\nSelect the top \\nk eigenvectors (principal components).\\nTransform the original data onto the new subspace.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q3\n",
    "'''\n",
    "PCA is a technique used for dimensionality reduction while preserving as much variance as possible. PCA transforms the data into a new coordinate \n",
    "system where the greatest variances are aligned with the new axes (principal components).\n",
    "\n",
    "Example:\n",
    "\n",
    "Suppose we have a dataset with two features: height and weight. We want to reduce it to one dimension using PCA.\n",
    "\n",
    "Standardize the data.\n",
    "Compute the covariance matrix.\n",
    "Compute the eigenvalues and eigenvectors of the covariance matrix.\n",
    "Select the top \n",
    "k eigenvectors (principal components).\n",
    "Transform the original data onto the new subspace.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdf2c49a-358f-415b-a118-f280de9c1075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPCA is related to feature extraction as it identifies new features (principal components) that capture the maximum variance in the data. These new \\nfeatures are linear combinations of the original features.\\n\\nExample:\\n\\nUsing the height and weight dataset:\\n\\nPerform PCA and find that the first principal component explains 90% of the variance.\\nUse the first principal component as the extracted feature, reducing the dataset from two dimensions to one while retaining most of the information.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q4\n",
    "'''\n",
    "PCA is related to feature extraction as it identifies new features (principal components) that capture the maximum variance in the data. These new \n",
    "features are linear combinations of the original features.\n",
    "\n",
    "Example:\n",
    "\n",
    "Using the height and weight dataset:\n",
    "\n",
    "Perform PCA and find that the first principal component explains 90% of the variance.\n",
    "Use the first principal component as the extracted feature, reducing the dataset from two dimensions to one while retaining most of the information.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4770f87-03c9-4193-8278-d7c0bd1a0506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   prise  rating  distance\n",
      "0      9       3         2\n",
      "1      9       3         6\n",
      "2      4       2         4\n",
      "3      8       1         5\n",
      "4      8       9         5\n",
      "5      1       5         4\n",
      "6      5       1         8\n",
      "7      3       7         2\n",
      "8      6       3         2\n",
      "9      3       5         8\n",
      "   prise  rating  distance\n",
      "0  1.000   0.250  0.000000\n",
      "1  1.000   0.250  0.666667\n",
      "2  0.375   0.125  0.333333\n",
      "3  0.875   0.000  0.500000\n",
      "4  0.875   1.000  0.500000\n",
      "5  0.000   0.500  0.333333\n",
      "6  0.500   0.000  1.000000\n",
      "7  0.250   0.750  0.000000\n",
      "8  0.625   0.250  0.000000\n",
      "9  0.250   0.500  1.000000\n"
     ]
    }
   ],
   "source": [
    "#Q5\n",
    "'''\n",
    "To preprocess the dataset with features such as price, rating, and delivery time:\n",
    "\n",
    "Identify the minimum and maximum values for each feature.\n",
    "Apply Min-Max scaling to each feature to transform the values to the range [0, 1].\n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "np.random.seed(100)\n",
    "\n",
    "\n",
    "def create_DF(string,n):\n",
    "    ls1 = {col: np.random.randint(1,10,n) for col in string}\n",
    "    df1 = pd.DataFrame(ls1)\n",
    "    df1.to_csv(\"df1.csv\",index=False)\n",
    "n=10\n",
    "cloumns = ['prise','rating','distance']\n",
    "create_DF(cloumns,n)\n",
    "df1 = pd.read_csv('df1.csv')\n",
    "df1\n",
    "def normalized(df):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns )\n",
    "    return scaled_df\n",
    "\n",
    "scaled_df =  normalized(df1)\n",
    "print(df1)\n",
    "print(scaled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92a6dbdd-6d7c-4742-a20c-19954389c465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTo reduce the dimensionality of the dataset with many features:\\n\\nStandardize the data.\\nPerform PCA to compute the principal components.\\nChoose the number of components that explain a significant amount of the variance (e.g., 95%).\\nTransform the dataset using these components, reducing the number of features while retaining most of the information.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q6\n",
    "'''\n",
    "To reduce the dimensionality of the dataset with many features:\n",
    "\n",
    "Standardize the data.\n",
    "Perform PCA to compute the principal components.\n",
    "Choose the number of components that explain a significant amount of the variance (e.g., 95%).\n",
    "Transform the dataset using these components, reducing the number of features while retaining most of the information.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f39b8826-d775-435b-8b95-8775d9bae791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.        ],\n",
       "       [-0.57894737],\n",
       "       [-0.05263158],\n",
       "       [ 0.47368421],\n",
       "       [ 1.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q7\n",
    "'''\n",
    "For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1.\n",
    "'''\n",
    "values= [1, 5, 10, 15, 20]\n",
    "values=np.array(values).reshape(-1,1)\n",
    "scaler = MinMaxScaler(feature_range=(-1,1) )\n",
    "\n",
    "scaled = scaler.fit_transform(values)\n",
    "scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf6f4b69-de21-4027-9471-8a78864cfdd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTo perform Feature Extraction using PCA on the dataset [height, weight, age, gender, blood pressure], standardize the data first. Apply PCA and examine \\nthe explained variance ratio of the principal components. Retain the components that cumulatively explain at least 95% of the variance. This typically \\nresults in fewer components than original features, simplifying the model while preserving most of the information. If the first three components \\nexplain 95% of the variance, retain those three.\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q8\n",
    "'''\n",
    "To perform Feature Extraction using PCA on the dataset [height, weight, age, gender, blood pressure], standardize the data first. Apply PCA and examine \n",
    "the explained variance ratio of the principal components. Retain the components that cumulatively explain at least 95% of the variance. This typically \n",
    "results in fewer components than original features, simplifying the model while preserving most of the information. If the first three components \n",
    "explain 95% of the variance, retain those three.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fc1b69-20ef-4ae3-9676-0d7cbd51c5ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
